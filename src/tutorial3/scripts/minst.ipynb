{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7., 0., 0., ..., 0., 0., 0.],\n",
       "       [2., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [9., 0., 0., ..., 0., 0., 0.],\n",
       "       [5., 0., 0., ..., 0., 0., 0.],\n",
       "       [9., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_size = 28 # width and length\n",
    "no_of_different_labels = 10 #  i.e. 0, 1, 2, 3, ..., 9\n",
    "image_pixels = image_size * image_size\n",
    "data_path = \"\"\n",
    "train_data = np.loadtxt(data_path + \"mnist_train.csv\", \n",
    "                        delimiter=\",\")\n",
    "test_data = np.loadtxt(data_path + \"mnist_test.csv\", \n",
    "                       delimiter=\",\") \n",
    "test_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fac = 0.99 / 255\n",
    "train_imgs = np.asfarray(train_data[:, 1:]) * fac + 0.01\n",
    "test_imgs = np.asfarray(test_data[:, 1:]) * fac + 0.01\n",
    "\n",
    "train_labels = np.asfarray(train_data[:, :1])\n",
    "test_labels = np.asfarray(test_data[:, :1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label:  0  in one-hot representation:  [1 0 0 0 0 0 0 0 0 0]\n",
      "label:  1  in one-hot representation:  [0 1 0 0 0 0 0 0 0 0]\n",
      "label:  2  in one-hot representation:  [0 0 1 0 0 0 0 0 0 0]\n",
      "label:  3  in one-hot representation:  [0 0 0 1 0 0 0 0 0 0]\n",
      "label:  4  in one-hot representation:  [0 0 0 0 1 0 0 0 0 0]\n",
      "label:  5  in one-hot representation:  [0 0 0 0 0 1 0 0 0 0]\n",
      "label:  6  in one-hot representation:  [0 0 0 0 0 0 1 0 0 0]\n",
      "label:  7  in one-hot representation:  [0 0 0 0 0 0 0 1 0 0]\n",
      "label:  8  in one-hot representation:  [0 0 0 0 0 0 0 0 1 0]\n",
      "label:  9  in one-hot representation:  [0 0 0 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "lr = np.arange(10)\n",
    "\n",
    "for label in range(10):\n",
    "    one_hot = (lr==label).astype(np.int)\n",
    "    print(\"label: \", label, \" in one-hot representation: \", one_hot)\n",
    "    lr = np.arange(no_of_different_labels)\n",
    "\n",
    "# transform labels into one hot representation\n",
    "train_labels_one_hot = (lr==train_labels).astype(np.float)\n",
    "test_labels_one_hot = (lr==test_labels).astype(np.float)\n",
    "\n",
    "# we don't want zeroes and ones in the labels neither:\n",
    "train_labels_one_hot[train_labels_one_hot==0] = 0.01\n",
    "train_labels_one_hot[train_labels_one_hot==1] = 0.99\n",
    "test_labels_one_hot[test_labels_one_hot==0] = 0.01\n",
    "test_labels_one_hot[test_labels_one_hot==1] = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "@np.vectorize\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.e ** -x)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "activation_function = sigmoid\n",
    "\n",
    "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
    "    return truncnorm((low - mean) / sd,\n",
    "                     (upp - mean) / sd, \n",
    "                     loc=mean, \n",
    "                     scale=sd)\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "        \n",
    "    \n",
    "    def __init__(self, \n",
    "                 network_structure, # ie. [input_nodes, hidden1_nodes, ... , hidden_n_nodes, output_nodes]\n",
    "                 learning_rate,\n",
    "                 bias=None\n",
    "                ):  \n",
    "\n",
    "        self.structure = network_structure\n",
    "        self.learning_rate = learning_rate \n",
    "        self.bias = bias\n",
    "        self.create_weight_matrices()\n",
    "\n",
    "    \n",
    "    \n",
    "    def create_weight_matrices(self):\n",
    "        X = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        \n",
    "        bias_node = 1 if self.bias else 0\n",
    "        self.weights_matrices = []    \n",
    "        layer_index = 1\n",
    "        no_of_layers = len(self.structure)\n",
    "        while layer_index < no_of_layers:\n",
    "            nodes_in = self.structure[layer_index-1]\n",
    "            nodes_out = self.structure[layer_index]\n",
    "            n = (nodes_in + bias_node) * nodes_out\n",
    "            # initialize the weight matrix of each layer\n",
    "            rad = 1 / np.sqrt(nodes_in)\n",
    "            X = truncated_normal(mean=2, sd=1, low=-rad, upp=rad)\n",
    "            wm = X.rvs(n).reshape((nodes_out, nodes_in + bias_node))\n",
    "            #attach to the parameter\n",
    "            self.weights_matrices.append(wm)\n",
    "            layer_index += 1\n",
    "\n",
    "        \n",
    "        \n",
    "    def train_single(self, input_vector, target_vector):\n",
    "        # input_vector and target_vector can be tuple, list or ndarray\n",
    "                                       \n",
    "        no_of_layers = len(self.structure)        \n",
    "        input_vector = np.array(input_vector, ndmin=2).T\n",
    "        \n",
    "        # forward pass\n",
    "        layer_index = 0\n",
    "        # The output/input vectors of the various layers:\n",
    "        res_vectors = [input_vector]          \n",
    "        while layer_index < no_of_layers - 1:\n",
    "            #take the last element of res_vector\n",
    "            in_vector = res_vectors[-1]\n",
    "            if self.bias:\n",
    "                # adding bias node to the end of the 'input'_vector\n",
    "                in_vector = np.concatenate( (in_vector, \n",
    "                                             [[self.bias]]) )\n",
    "                res_vectors[-1] = in_vector\n",
    "            \n",
    "            x = np.dot(self.weights_matrices[layer_index], in_vector)\n",
    "            if layer_index == no_of_layers-2:\n",
    "                out_vector = softmax(x)\n",
    "            else:\n",
    "                out_vector = activation_function(x)\n",
    "            res_vectors.append(out_vector)   \n",
    "            layer_index += 1\n",
    "        \n",
    "        # backward pass\n",
    "        layer_index = no_of_layers - 1\n",
    "        target_vector = np.array(target_vector, ndmin=2).T\n",
    "         # The input vectors to the various layers\n",
    "        output_errors = target_vector - out_vector  \n",
    "        while layer_index > 0:\n",
    "            out_vector = res_vectors[layer_index]\n",
    "            in_vector = res_vectors[layer_index-1]\n",
    "\n",
    "            if self.bias and not layer_index==(no_of_layers-1):\n",
    "                out_vector = out_vector[:-1,:].copy()\n",
    "            if layer_index==no_of_layers - 1:\n",
    "                ovn = out_vector.reshape(out_vector.size,)\n",
    "                si_sj = - ovn * ovn.reshape(self.structure[-1], 1)\n",
    "                s_der = np.diag(ovn) + si_sj\n",
    "                tmp =  s_der @ output_errors \n",
    "            else:    \n",
    "                tmp = output_errors * out_vector * (1.0 - out_vector)     \n",
    "            tmp = np.dot(tmp, in_vector.T)\n",
    "            self.weights_matrices[layer_index-1] += self.learning_rate * tmp\n",
    "            \n",
    "            output_errors = np.dot(self.weights_matrices[layer_index-1].T, \n",
    "                                   output_errors)\n",
    "            if self.bias:\n",
    "                output_errors = output_errors[:-1,:]\n",
    "            layer_index -= 1\n",
    "            \n",
    "\n",
    "       \n",
    "\n",
    "    def train(self, data_array, \n",
    "              labels_one_hot_array,\n",
    "              epochs=1,\n",
    "              intermediate_results=False):\n",
    "        intermediate_weights = []\n",
    "        for epoch in range(epochs):  \n",
    "            for i in range(len(data_array)):\n",
    "                self.train_single(data_array[i], labels_one_hot_array[i])\n",
    "            if intermediate_results:\n",
    "                intermediate_weights.append((self.wih.copy(), \n",
    "                                             self.who.copy()))\n",
    "        return intermediate_weights      \n",
    "        \n",
    "\n",
    "               \n",
    "    \n",
    "    def run(self, input_vector):\n",
    "        # input_vector can be tuple, list or ndarray\n",
    "\n",
    "        no_of_layers = len(self.structure)\n",
    "        if self.bias:\n",
    "            # adding bias node to the end of the inpuy_vector\n",
    "            input_vector = np.concatenate( (input_vector, [self.bias]) )\n",
    "        in_vector = np.array(input_vector, ndmin=2).T\n",
    "\n",
    "        layer_index = 1\n",
    "        # The input vectors to the various layers\n",
    "        while layer_index < no_of_layers:\n",
    "            x = np.dot(self.weights_matrices[layer_index-1], \n",
    "                       in_vector)\n",
    "            out_vector = activation_function(x)\n",
    "            \n",
    "            # input vector for next layer\n",
    "            in_vector = out_vector\n",
    "            if self.bias:\n",
    "                in_vector = np.concatenate( (in_vector, \n",
    "                                             [[self.bias]]) )            \n",
    "            \n",
    "            layer_index += 1\n",
    "  \n",
    "    \n",
    "        return out_vector\n",
    "    \n",
    "    def evaluate(self, data, labels):\n",
    "        corrects, wrongs = 0, 0\n",
    "        for i in range(len(data)):\n",
    "            res = self.run(data[i])\n",
    "            res_max = res.argmax()\n",
    "            if res_max == labels[i]:\n",
    "                corrects += 1\n",
    "            else:\n",
    "                wrongs += 1\n",
    "        return corrects, wrongs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "ANN = NeuralNetwork(network_structure=[image_pixels, 80, 80, 10],\n",
    "                               learning_rate=0.01,\n",
    "                               bias=None)\n",
    "    \n",
    "    \n",
    "ANN.train(train_imgs, train_labels_one_hot, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy train:  0.96\n",
      "accuracy: test 0.9571\n"
     ]
    }
   ],
   "source": [
    "corrects, wrongs = ANN.evaluate(train_imgs, train_labels)\n",
    "print(\"accuracy train: \", corrects / ( corrects + wrongs))\n",
    "corrects, wrongs = ANN.evaluate(test_imgs, test_labels)\n",
    "print(\"accuracy: test\", corrects / ( corrects + wrongs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
