{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103, 2)\n",
      "(12, 2)\n",
      "(103, 2)\n",
      "(12, 2)\n",
      "[-0.36009029  0.48012084]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "input_size = 2\n",
    "output_size = 2\n",
    "data_path = \"\"\n",
    "Data = np.loadtxt(\"data.txt\",delimiter=\",\")\n",
    "#test_data = np.loadtxt(data_path + \"mnist_test.csv\", \n",
    "                      # delimiter=\",\") \n",
    "X_data = Data[:,1:3]\n",
    "Y_data = Data[:,3:]\n",
    "N = X_data.shape[0]\n",
    "val_portion = 0.1;\n",
    "X_train = X_data[:int((1-val_portion)*N),:]\n",
    "X_test = X_data[int((1-val_portion)*N):,:]\n",
    "Y_train = Y_data[:int((1-val_portion)*N),:]\n",
    "Y_test = Y_data[int((1-val_portion)*N):,:]\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)\n",
    "print(Y_train[46,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9995073  0.56773131]\n",
      " [0.90334049 0.97141664]\n",
      " [0.44189919 0.30870002]\n",
      " [0.59269549 0.30869998]\n",
      " [0.74301238 0.33186855]\n",
      " [0.68791534 0.99121033]\n",
      " [0.53986606 0.30869998]\n",
      " [0.04854136 0.66437511]\n",
      " [0.91068801 0.74365826]\n",
      " [0.33869694 0.72300427]\n",
      " [0.37570793 0.30869998]\n",
      " [0.92083856 0.5686089 ]\n",
      " [0.42908457 0.67040883]\n",
      " [0.54480806 0.69820173]\n",
      " [0.99952241 0.6245216 ]\n",
      " [0.90477344 0.38198758]\n",
      " [0.32752716 0.48565033]\n",
      " [0.34061749 0.69966807]\n",
      " [0.28120715 0.65197997]\n",
      " [0.10387831 0.30870004]\n",
      " [0.12220589 0.30869998]\n",
      " [0.99926462 0.3087    ]\n",
      " [0.9995062  0.46879335]\n",
      " [0.91302392 0.30869992]\n",
      " [0.23395387 0.75083007]\n",
      " [0.65553837 0.38829311]\n",
      " [0.29060761 0.42941627]\n",
      " [0.50829693 0.94620167]\n",
      " [0.3394804  0.30869997]\n",
      " [0.11029739 0.41458968]\n",
      " [0.77660675 0.43003628]\n",
      " [0.84809875 0.96422814]\n",
      " [0.71517001 0.30869997]\n",
      " [0.23445199 0.90560387]\n",
      " [0.99949748 0.82117336]\n",
      " [0.95367011 0.30869996]\n",
      " [0.46128568 0.48479148]\n",
      " [0.92678329 0.49932693]\n",
      " [0.81654386 0.58652331]\n",
      " [0.09174408 0.53395002]\n",
      " [0.70249059 0.30869984]\n",
      " [0.28624779 0.75252213]\n",
      " [0.42599918 0.48190351]\n",
      " [0.70492182 0.44001253]\n",
      " [0.5754468  0.30870001]\n",
      " [0.39755823 0.38375009]\n",
      " [0.48558531 0.78012084]\n",
      " [0.73417191 0.71188449]\n",
      " [0.13685969 0.63350247]\n",
      " [0.59664345 0.30869987]\n",
      " [0.12592564 0.85320245]\n",
      " [0.27730518 0.30870006]\n",
      " [0.12163079 0.93482744]\n",
      " [0.20948437 0.88133548]\n",
      " [0.870661   0.47444505]\n",
      " [0.44893743 0.42119966]\n",
      " [0.14342863 0.31771384]\n",
      " [0.31727678 0.57257591]\n",
      " [0.22705301 0.8531643 ]\n",
      " [0.14520511 0.32520076]\n",
      " [0.26583937 0.40997085]\n",
      " [0.34092019 0.64653002]\n",
      " [0.73466873 0.8285064 ]\n",
      " [0.83120125 0.32830537]\n",
      " [0.75280106 0.37848568]\n",
      " [0.30839326 0.78197341]\n",
      " [0.72927907 0.71548327]\n",
      " [0.56972216 0.79552315]\n",
      " [0.41035607 0.94332587]\n",
      " [0.10800729 0.51413736]\n",
      " [0.66008216 0.4887254 ]\n",
      " [0.99948266 0.42733804]\n",
      " [0.95150938 0.3086998 ]\n",
      " [0.6818988  0.36341925]\n",
      " [0.46005217 0.51200053]\n",
      " [0.12932021 0.30869996]\n",
      " [0.66775771 0.30869979]\n",
      " [0.82069739 0.36518494]\n",
      " [0.62251629 0.84428077]\n",
      " [0.69491401 0.57239591]\n",
      " [0.8890149  0.54090363]\n",
      " [0.78859333 0.60184379]\n",
      " [0.83535001 0.72520508]\n",
      " [0.70145477 0.58211161]\n",
      " [0.82254459 0.5458062 ]\n",
      " [0.85518752 0.3308348 ]\n",
      " [0.8566404  0.31081191]\n",
      " [0.9768682  0.35814233]\n",
      " [0.84861472 0.81029783]\n",
      " [0.37128335 0.68012406]\n",
      " [0.23908131 0.30869993]\n",
      " [0.20275416 0.58672743]\n",
      " [0.97361614 0.30870003]\n",
      " [0.83661896 0.79963921]\n",
      " [0.87152534 0.88154571]\n",
      " [0.76958824 0.86492347]\n",
      " [0.55046254 0.47811772]\n",
      " [0.86853924 0.38849011]\n",
      " [0.22924485 0.3086998 ]\n",
      " [0.71370061 0.36552551]\n",
      " [0.86554711 0.32416997]\n",
      " [0.78593166 0.36441031]\n",
      " [0.37005354 0.88775759]]\n"
     ]
    }
   ],
   "source": [
    "fac_x = 0.99 / 320\n",
    "fac_y = 0.99 / 240\n",
    "train_X = np.zeros((X_train.shape))\n",
    "train_Y = np.zeros(Y_train.shape)\n",
    "test_X = np.zeros((X_test.shape))\n",
    "test_Y = np.zeros(Y_test.shape)\n",
    "\n",
    "train_X[:,0] = np.asfarray(X_train[:, 0]) * fac_x + 0.01 - 0.5\n",
    "train_X[:,1] = np.asfarray(X_train[:, 1]) * fac_y + 0.01 -0.5\n",
    "test_X[:,0] = np.asfarray(X_test[:, 0]) * fac_x + 0.01 -0.5\n",
    "test_X[:,1] = np.asfarray(X_test[:, 1]) * fac_y + 0.01 -0.5\n",
    "\n",
    "train_Y[:,0] = ((np.asfarray(Y_train[:,0]) - (-0.7))/0.7)*1\n",
    "train_Y[:,1] = ((np.asfarray(Y_train[:,1]) - (-0.3))/1)*1\n",
    "test_Y[:,0] = (np.asfarray(Y_test[:,0]) - (-0.7))/0.7\n",
    "test_Y[:,1] = (np.asfarray(Y_test[:,1]) - (-0.3))/1\n",
    "\n",
    "print train_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlr = np.arange(10)\\n\\nfor label in range(10):\\n    one_hot = (lr==label).astype(np.int)\\n    print(\"label: \", label, \" in one-hot representation: \", one_hot)\\n    lr = np.arange(no_of_different_labels)\\n\\n# transform labels into one hot representation\\ntrain_labels_one_hot = (lr==train_labels).astype(np.float)\\ntest_labels_one_hot = (lr==test_labels).astype(np.float)\\n\\n# we don\\'t want zeroes and ones in the labels neither:\\ntrain_labels_one_hot[train_labels_one_hot==0] = 0.01\\ntrain_labels_one_hot[train_labels_one_hot==1] = 0.99\\ntest_labels_one_hot[test_labels_one_hot==0] = 0.01\\ntest_labels_one_hot[test_labels_one_hot==1] = 0.99\\n'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "lr = np.arange(10)\n",
    "\n",
    "for label in range(10):\n",
    "    one_hot = (lr==label).astype(np.int)\n",
    "    print(\"label: \", label, \" in one-hot representation: \", one_hot)\n",
    "    lr = np.arange(no_of_different_labels)\n",
    "\n",
    "# transform labels into one hot representation\n",
    "train_labels_one_hot = (lr==train_labels).astype(np.float)\n",
    "test_labels_one_hot = (lr==test_labels).astype(np.float)\n",
    "\n",
    "# we don't want zeroes and ones in the labels neither:\n",
    "train_labels_one_hot[train_labels_one_hot==0] = 0.01\n",
    "train_labels_one_hot[train_labels_one_hot==1] = 0.99\n",
    "test_labels_one_hot[test_labels_one_hot==0] = 0.01\n",
    "test_labels_one_hot[test_labels_one_hot==1] = 0.99\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "@np.vectorize\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.e ** -x)\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "activation_function = sigmoid\n",
    "\n",
    "def truncated_normal(mean=0, sd=1, low=0, upp=10):\n",
    "    return truncnorm((low - mean) / sd,\n",
    "                     (upp - mean) / sd, \n",
    "                     loc=mean, \n",
    "                     scale=sd)\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "        \n",
    "    \n",
    "    def __init__(self, \n",
    "                 network_structure, # ie. [input_nodes, hidden1_nodes, ... , hidden_n_nodes, output_nodes]\n",
    "                 learning_rate,\n",
    "                 bias=None\n",
    "                ):  \n",
    "\n",
    "        self.structure = network_structure\n",
    "        self.learning_rate = learning_rate \n",
    "        self.bias = bias\n",
    "        self.create_weight_matrices()\n",
    "\n",
    "    \n",
    "    \n",
    "    def create_weight_matrices(self):\n",
    "        X = truncated_normal(mean=2, sd=1, low=-0.5, upp=0.5)\n",
    "        \n",
    "        bias_node = 1 if self.bias else 0\n",
    "        self.weights_matrices = []    \n",
    "        layer_index = 1\n",
    "        no_of_layers = len(self.structure)\n",
    "        while layer_index < no_of_layers:\n",
    "            nodes_in = self.structure[layer_index-1]\n",
    "            nodes_out = self.structure[layer_index]\n",
    "            n = (nodes_in + bias_node) * nodes_out\n",
    "            # initialize the weight matrix of each layer\n",
    "            rad = 1 / np.sqrt(nodes_in)\n",
    "            X = truncated_normal(mean=2, sd=1, low=-rad, upp=rad)\n",
    "            wm = X.rvs(n).reshape((nodes_out, nodes_in + bias_node))\n",
    "            #attach to the parameter\n",
    "            self.weights_matrices.append(wm)\n",
    "            layer_index += 1\n",
    "\n",
    "        \n",
    "        \n",
    "    def train_single(self, input_vector, target_vector):\n",
    "        # input_vector and target_vector can be tuple, list or ndarray\n",
    "                                       \n",
    "        no_of_layers = len(self.structure)        \n",
    "        input_vector = np.array(input_vector, ndmin=2).T\n",
    "        \n",
    "        # forward pass\n",
    "        layer_index = 0\n",
    "        # The output/input vectors of the various layers:\n",
    "        res_vectors = [input_vector]          \n",
    "        while layer_index < no_of_layers - 1:\n",
    "            #take the last element of res_vector\n",
    "            in_vector = res_vectors[-1]\n",
    "            if self.bias:\n",
    "                # adding bias node to the end of the 'input'_vector\n",
    "                in_vector = np.concatenate( (in_vector, \n",
    "                                             [[self.bias]]) )\n",
    "                res_vectors[-1] = in_vector\n",
    "            x = np.dot(self.weights_matrices[layer_index], in_vector)\n",
    "            out_vector = activation_function(x)\n",
    "            res_vectors.append(out_vector)   \n",
    "            layer_index += 1\n",
    "            \n",
    "        #out_vector = np.tanh(x)\n",
    "        \n",
    "        \n",
    "        # backward pass\n",
    "        layer_index = no_of_layers - 1\n",
    "        target_vector = np.array(target_vector, ndmin=2).T\n",
    "        # The input vectors to the various layers\n",
    "        # output_errors = target_vector - out_vector\n",
    "        output_errors= 2*(target_vector-out_vector)\n",
    "        #tmp = 1 - out_vector*out_vector\n",
    "        while layer_index > 0:\n",
    "            out_vector = res_vectors[layer_index]\n",
    "            in_vector = res_vectors[layer_index-1]\n",
    "\n",
    "            if self.bias and not layer_index==(no_of_layers-1):\n",
    "                out_vector = out_vector[:-1,:].copy()\n",
    "                \n",
    "            tmp = output_errors * out_vector * (1.0 - out_vector)  \n",
    "            \n",
    "            #print(\"grad sig:\",tmp)\n",
    "            tmp = np.dot(tmp, in_vector.T)  \n",
    "            #print(\"layer_index:\", layer_index,\"grads:\", tmp)\n",
    "            #print (\"weight before \",self.weights_matrices[layer_index-1])\n",
    "            self.weights_matrices[layer_index-1] += self.learning_rate * tmp\n",
    "            #print (\"weight after \", self.weights_matrices[layer_index-1])\n",
    "            output_errors = np.dot(self.weights_matrices[layer_index-1].T, \n",
    "                                   output_errors)\n",
    "            if self.bias:\n",
    "                output_errors = output_errors[:-1,:]\n",
    "            layer_index -= 1\n",
    "            \n",
    "\n",
    "       \n",
    "\n",
    "    def train(self, data_array, \n",
    "              target_array,\n",
    "              epochs=1,\n",
    "              intermediate_results=False):\n",
    "        intermediate_weights = []\n",
    "        for epoch in range(epochs):  \n",
    "            for i in range(len(data_array)):\n",
    "                self.train_single(data_array[i], target_array[i])\n",
    "            if intermediate_results:\n",
    "                intermediate_weights.append((self.wih.copy(), \n",
    "                                             self.who.copy()))\n",
    "            e, e_p = self.evaluate(data_array, target_array)\n",
    "            print(\"epoch:\", epoch, \"MSE train: \", e)\n",
    "            \n",
    "        return intermediate_weights      \n",
    "        \n",
    "\n",
    "               \n",
    "    \n",
    "    def run(self, input_vector):\n",
    "        # input_vector can be tuple, list or ndarray\n",
    "\n",
    "        no_of_layers = len(self.structure)\n",
    "        if self.bias:\n",
    "            # adding bias node to the end of the input_vector\n",
    "            input_vector = np.concatenate( (input_vector, np.ones((input_vector.shape[0],1))),axis=1 )\n",
    "        in_vector = np.array(input_vector, ndmin=2).T\n",
    "\n",
    "        layer_index = 1\n",
    "        # The input vectors to the various layers\n",
    "        while layer_index < no_of_layers:\n",
    "            x = np.dot(self.weights_matrices[layer_index-1], \n",
    "                       in_vector)\n",
    "            out_vector = activation_function(x)\n",
    "            \n",
    "            # input vector for next layer\n",
    "            in_vector = out_vector\n",
    "            if self.bias: \n",
    "                in_vector = np.concatenate( (in_vector, \n",
    "                                             np.ones((1,in_vector.shape[1])) ),axis=0)            \n",
    "            \n",
    "            layer_index += 1\n",
    "  \n",
    "    \n",
    "        return out_vector\n",
    "    \n",
    "    def evaluate(self, data, y_real):\n",
    "        y_pred = (self.run(data)).T\n",
    "        e=np.mean(np.multiply(y_real-y_pred, y_real-y_pred))\n",
    "        e_prime= 2*(y_pred-y_real)/y_real.size\n",
    "        return e, y_pred\n",
    "    \n",
    "    def save_paras(self):\n",
    "        return self.weights_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('epoch:', 0, 'MSE train: ', 0.06772072287489896)\n",
      "('epoch:', 1, 'MSE train: ', 0.06754044707151197)\n",
      "('epoch:', 2, 'MSE train: ', 0.06734294454040461)\n",
      "('epoch:', 3, 'MSE train: ', 0.06711896173008261)\n",
      "('epoch:', 4, 'MSE train: ', 0.06685735177031643)\n",
      "('epoch:', 5, 'MSE train: ', 0.0665435120267433)\n",
      "('epoch:', 6, 'MSE train: ', 0.06615677897709156)\n",
      "('epoch:', 7, 'MSE train: ', 0.06566637387924895)\n",
      "('epoch:', 8, 'MSE train: ', 0.06502486169538921)\n",
      "('epoch:', 9, 'MSE train: ', 0.06415746556680851)\n",
      "('epoch:', 10, 'MSE train: ', 0.0629446478584034)\n",
      "('epoch:', 11, 'MSE train: ', 0.06119477481757337)\n",
      "('epoch:', 12, 'MSE train: ', 0.05860995819735474)\n",
      "('epoch:', 13, 'MSE train: ', 0.054789421812877104)\n",
      "('epoch:', 14, 'MSE train: ', 0.04942840800008994)\n",
      "('epoch:', 15, 'MSE train: ', 0.042869906334883974)\n",
      "('epoch:', 16, 'MSE train: ', 0.03646355513456922)\n",
      "('epoch:', 17, 'MSE train: ', 0.03163513179034591)\n",
      "('epoch:', 18, 'MSE train: ', 0.02863331305763084)\n",
      "('epoch:', 19, 'MSE train: ', 0.02682922679840528)\n",
      "('epoch:', 20, 'MSE train: ', 0.025579861244979882)\n",
      "('epoch:', 21, 'MSE train: ', 0.024498576749611412)\n",
      "('epoch:', 22, 'MSE train: ', 0.02337730408967788)\n",
      "('epoch:', 23, 'MSE train: ', 0.022086035178963983)\n",
      "('epoch:', 24, 'MSE train: ', 0.020519723786335223)\n",
      "('epoch:', 25, 'MSE train: ', 0.018586407828383424)\n",
      "('epoch:', 26, 'MSE train: ', 0.0162354615407655)\n",
      "('epoch:', 27, 'MSE train: ', 0.013528301918262858)\n",
      "('epoch:', 28, 'MSE train: ', 0.010708929743203035)\n",
      "('epoch:', 29, 'MSE train: ', 0.008159698119376835)\n",
      "('epoch:', 30, 'MSE train: ', 0.006199491041482843)\n",
      "('epoch:', 31, 'MSE train: ', 0.004909669126288492)\n",
      "('epoch:', 32, 'MSE train: ', 0.004164465333633989)\n",
      "('epoch:', 33, 'MSE train: ', 0.0037747663398796536)\n",
      "('epoch:', 34, 'MSE train: ', 0.0035859584776122004)\n",
      "('epoch:', 35, 'MSE train: ', 0.0035002768621466844)\n",
      "('epoch:', 36, 'MSE train: ', 0.0034640233301969844)\n",
      "('epoch:', 37, 'MSE train: ', 0.0034501122588338536)\n",
      "('epoch:', 38, 'MSE train: ', 0.0034456490987187067)\n",
      "('epoch:', 39, 'MSE train: ', 0.0034447796242752828)\n",
      "('epoch:', 40, 'MSE train: ', 0.0034449735150667124)\n",
      "('epoch:', 41, 'MSE train: ', 0.0034452012357439575)\n",
      "('epoch:', 42, 'MSE train: ', 0.0034450740213656053)\n",
      "('epoch:', 43, 'MSE train: ', 0.0034444523962286673)\n",
      "('epoch:', 44, 'MSE train: ', 0.00344327596485737)\n",
      "('epoch:', 45, 'MSE train: ', 0.003441494955484662)\n",
      "('epoch:', 46, 'MSE train: ', 0.0034390470035603974)\n",
      "('epoch:', 47, 'MSE train: ', 0.003435852983110463)\n",
      "('epoch:', 48, 'MSE train: ', 0.0034318200611366226)\n",
      "('epoch:', 49, 'MSE train: ', 0.003426846866589144)\n",
      "('epoch:', 50, 'MSE train: ', 0.0034208287482303795)\n",
      "('epoch:', 51, 'MSE train: ', 0.003413662464875791)\n",
      "('epoch:', 52, 'MSE train: ', 0.003405250218037451)\n",
      "('epoch:', 53, 'MSE train: ', 0.003395503130820872)\n",
      "('epoch:', 54, 'MSE train: ', 0.003384344300301101)\n",
      "('epoch:', 55, 'MSE train: ', 0.0033717114996999788)\n",
      "('epoch:', 56, 'MSE train: ', 0.003357559529986838)\n",
      "('epoch:', 57, 'MSE train: ', 0.0033418621463800636)\n",
      "('epoch:', 58, 'MSE train: ', 0.0033246134335044105)\n",
      "('epoch:', 59, 'MSE train: ', 0.003305828487882903)\n",
      "('epoch:', 60, 'MSE train: ', 0.0032855432947619238)\n",
      "('epoch:', 61, 'MSE train: ', 0.003263813754239345)\n",
      "('epoch:', 62, 'MSE train: ', 0.0032407139041113297)\n",
      "('epoch:', 63, 'MSE train: ', 0.0032163334804506455)\n",
      "('epoch:', 64, 'MSE train: ', 0.0031907750274813027)\n",
      "('epoch:', 65, 'MSE train: ', 0.0031641507991302786)\n",
      "('epoch:', 66, 'MSE train: ', 0.003136579682012917)\n",
      "('epoch:', 67, 'MSE train: ', 0.003108184322776001)\n",
      "('epoch:', 68, 'MSE train: ', 0.0030790885787004524)\n",
      "('epoch:', 69, 'MSE train: ', 0.0030494153468615145)\n",
      "('epoch:', 70, 'MSE train: ', 0.0030192847764558776)\n",
      "('epoch:', 71, 'MSE train: ', 0.0029888128363070947)\n",
      "('epoch:', 72, 'MSE train: ', 0.0029581101939707153)\n",
      "('epoch:', 73, 'MSE train: ', 0.0029272813596976028)\n",
      "('epoch:', 74, 'MSE train: ', 0.0028964240524503885)\n",
      "('epoch:', 75, 'MSE train: ', 0.002865628751895551)\n",
      "('epoch:', 76, 'MSE train: ', 0.0028349784071468155)\n",
      "('epoch:', 77, 'MSE train: ', 0.0028045482788167193)\n",
      "('epoch:', 78, 'MSE train: ', 0.0027744058954004107)\n",
      "('epoch:', 79, 'MSE train: ', 0.0027446111083706693)\n",
      "('epoch:', 80, 'MSE train: ', 0.0027152162329022703)\n",
      "('epoch:', 81, 'MSE train: ', 0.002686266263089603)\n",
      "('epoch:', 82, 'MSE train: ', 0.002657799151989218)\n",
      "('epoch:', 83, 'MSE train: ', 0.0026298461478509376)\n",
      "('epoch:', 84, 'MSE train: ', 0.002602432178519751)\n",
      "('epoch:', 85, 'MSE train: ', 0.002575576276238676)\n",
      "('epoch:', 86, 'MSE train: ', 0.0025492920350410845)\n",
      "('epoch:', 87, 'MSE train: ', 0.0025235880927045695)\n",
      "('epoch:', 88, 'MSE train: ', 0.0024984686289788007)\n",
      "('epoch:', 89, 'MSE train: ', 0.0024739338716227295)\n",
      "('epoch:', 90, 'MSE train: ', 0.0024499806017927984)\n",
      "('epoch:', 91, 'MSE train: ', 0.0024266026505765765)\n",
      "('epoch:', 92, 'MSE train: ', 0.00240379137898843)\n",
      "('epoch:', 93, 'MSE train: ', 0.0023815361345203806)\n",
      "('epoch:', 94, 'MSE train: ', 0.002359824678329694)\n",
      "('epoch:', 95, 'MSE train: ', 0.0023386435782844247)\n",
      "('epoch:', 96, 'MSE train: ', 0.0023179785643120962)\n",
      "('epoch:', 97, 'MSE train: ', 0.0022978148437397586)\n",
      "('epoch:', 98, 'MSE train: ', 0.002278137375518024)\n",
      "('epoch:', 99, 'MSE train: ', 0.0022589311033411907)\n",
      "('epoch:', 100, 'MSE train: ', 0.0022401811486750414)\n",
      "('epoch:', 101, 'MSE train: ', 0.002221872965560615)\n",
      "('epoch:', 102, 'MSE train: ', 0.00220399245976254)\n",
      "('epoch:', 103, 'MSE train: ', 0.0021865260753697285)\n",
      "('epoch:', 104, 'MSE train: ', 0.0021694608523362374)\n",
      "('epoch:', 105, 'MSE train: ', 0.0021527844586782022)\n",
      "('epoch:', 106, 'MSE train: ', 0.002136485201130802)\n",
      "('epoch:', 107, 'MSE train: ', 0.002120552018032145)\n",
      "('epoch:', 108, 'MSE train: ', 0.002104974458056704)\n",
      "('epoch:', 109, 'MSE train: ', 0.0020897426481887864)\n",
      "('epoch:', 110, 'MSE train: ', 0.002074847254026883)\n",
      "('epoch:', 111, 'MSE train: ', 0.002060279435163061)\n",
      "('epoch:', 112, 'MSE train: ', 0.002046030798007217)\n",
      "('epoch:', 113, 'MSE train: ', 0.0020320933480421346)\n",
      "('epoch:', 114, 'MSE train: ', 0.002018459443117198)\n",
      "('epoch:', 115, 'MSE train: ', 0.0020051217490296125)\n",
      "('epoch:', 116, 'MSE train: ', 0.0019920731983117295)\n",
      "('epoch:', 117, 'MSE train: ', 0.001979306952848579)\n",
      "('epoch:', 118, 'MSE train: ', 0.0019668163706951032)\n",
      "('epoch:', 119, 'MSE train: ', 0.0019545949772493527)\n",
      "('epoch:', 120, 'MSE train: ', 0.0019426364407653825)\n",
      "('epoch:', 121, 'MSE train: ', 0.00193093455205555)\n",
      "('epoch:', 122, 'MSE train: ', 0.0019194832081327361)\n",
      "('epoch:', 123, 'MSE train: ', 0.001908276399474689)\n",
      "('epoch:', 124, 'MSE train: ', 0.0018973082005501996)\n",
      "('epoch:', 125, 'MSE train: ', 0.001886572763225955)\n",
      "('epoch:', 126, 'MSE train: ', 0.0018760643126689116)\n",
      "('epoch:', 127, 'MSE train: ', 0.0018657771453679992)\n",
      "('epoch:', 128, 'MSE train: ', 0.0018557056289172946)\n",
      "('epoch:', 129, 'MSE train: ', 0.0018458442032274998)\n",
      "('epoch:', 130, 'MSE train: ', 0.0018361873828611412)\n",
      "('epoch:', 131, 'MSE train: ', 0.0018267297602174116)\n",
      "('epoch:', 132, 'MSE train: ', 0.001817466009323514)\n",
      "('epoch:', 133, 'MSE train: ', 0.0018083908900197222)\n",
      "('epoch:', 134, 'MSE train: ', 0.0017994992523542664)\n",
      "('epoch:', 135, 'MSE train: ', 0.0017907860410312237)\n",
      "('epoch:', 136, 'MSE train: ', 0.0017822462997795214)\n",
      "('epoch:', 137, 'MSE train: ', 0.0017738751755338852)\n",
      "('epoch:', 138, 'MSE train: ', 0.001765667922338977)\n",
      "('epoch:', 139, 'MSE train: ', 0.001757619904906301)\n",
      "('epoch:', 140, 'MSE train: ', 0.0017497266017696767)\n",
      "('epoch:', 141, 'MSE train: ', 0.001741983607999479)\n",
      "('epoch:', 142, 'MSE train: ', 0.0017343866374484574)\n",
      "('epoch:', 143, 'MSE train: ', 0.0017269315245129503)\n",
      "('epoch:', 144, 'MSE train: ', 0.0017196142254030052)\n",
      "('epoch:', 145, 'MSE train: ', 0.0017124308189231305)\n",
      "('epoch:', 146, 'MSE train: ', 0.0017053775067725572)\n",
      "('epoch:', 147, 'MSE train: ', 0.0016984506133799428)\n",
      "('epoch:', 148, 'MSE train: ', 0.0016916465852924749)\n",
      "('epoch:', 149, 'MSE train: ', 0.0016849619901435522)\n",
      "('epoch:', 150, 'MSE train: ', 0.0016783935152265203)\n",
      "('epoch:', 151, 'MSE train: ', 0.0016719379657045609)\n",
      "('epoch:', 152, 'MSE train: ', 0.0016655922624888022)\n",
      "('epoch:', 153, 'MSE train: ', 0.001659353439817934)\n",
      "('epoch:', 154, 'MSE train: ', 0.0016532186425734377)\n",
      "('epoch:', 155, 'MSE train: ', 0.001647185123364713)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('epoch:', 156, 'MSE train: ', 0.001641250239418217)\n",
      "('epoch:', 157, 'MSE train: ', 0.0016354114493040404)\n",
      "('epoch:', 158, 'MSE train: ', 0.001629666309532446)\n",
      "('epoch:', 159, 'MSE train: ', 0.0016240124710515493)\n",
      "('epoch:', 160, 'MSE train: ', 0.001618447675675859)\n",
      "('epoch:', 161, 'MSE train: ', 0.0016129697524735712)\n",
      "('epoch:', 162, 'MSE train: ', 0.0016075766141386842)\n",
      "('epoch:', 163, 'MSE train: ', 0.0016022662533719005)\n",
      "('epoch:', 164, 'MSE train: ', 0.001597036739292211)\n",
      "('epoch:', 165, 'MSE train: ', 0.001591886213898849)\n",
      "('epoch:', 166, 'MSE train: ', 0.0015868128886011243)\n",
      "('epoch:', 167, 'MSE train: ', 0.0015818150408314709)\n",
      "('epoch:', 168, 'MSE train: ', 0.0015768910107548952)\n",
      "('epoch:', 169, 'MSE train: ', 0.0015720391980859297)\n",
      "('epoch:', 170, 'MSE train: ', 0.0015672580590221882)\n",
      "('epoch:', 171, 'MSE train: ', 0.0015625461033017306)\n",
      "('epoch:', 172, 'MSE train: ', 0.0015579018913896478)\n",
      "('epoch:', 173, 'MSE train: ', 0.0015533240317975887)\n",
      "('epoch:', 174, 'MSE train: ', 0.001548811178538442)\n",
      "('epoch:', 175, 'MSE train: ', 0.0015443620287169545)\n",
      "('epoch:', 176, 'MSE train: ', 0.0015399753202558146)\n",
      "('epoch:', 177, 'MSE train: ', 0.0015356498297556009)\n",
      "('epoch:', 178, 'MSE train: ', 0.0015313843704859854)\n",
      "('epoch:', 179, 'MSE train: ', 0.001527177790504777)\n",
      "('epoch:', 180, 'MSE train: ', 0.0015230289709005952)\n",
      "('epoch:', 181, 'MSE train: ', 0.0015189368241544231)\n",
      "('epoch:', 182, 'MSE train: ', 0.0015149002926147918)\n",
      "('epoch:', 183, 'MSE train: ', 0.0015109183470809397)\n",
      "('epoch:', 184, 'MSE train: ', 0.0015069899854880911)\n",
      "('epoch:', 185, 'MSE train: ', 0.0015031142316887587)\n",
      "('epoch:', 186, 'MSE train: ', 0.0014992901343239132)\n",
      "('epoch:', 187, 'MSE train: ', 0.00149551676577786)\n",
      "('epoch:', 188, 'MSE train: ', 0.0014917932212106968)\n",
      "('epoch:', 189, 'MSE train: ', 0.0014881186176623382)\n",
      "('epoch:', 190, 'MSE train: ', 0.0014844920932222758)\n",
      "('epoch:', 191, 'MSE train: ', 0.001480912806259427)\n",
      "('epoch:', 192, 'MSE train: ', 0.0014773799347066793)\n",
      "('epoch:', 193, 'MSE train: ', 0.0014738926753949974)\n",
      "('epoch:', 194, 'MSE train: ', 0.0014704502434322506)\n",
      "('epoch:', 195, 'MSE train: ', 0.0014670518716222245)\n",
      "('epoch:', 196, 'MSE train: ', 0.0014636968099195902)\n",
      "('epoch:', 197, 'MSE train: ', 0.0014603843249169344)\n",
      "('epoch:', 198, 'MSE train: ', 0.0014571136993602484)\n",
      "('epoch:', 199, 'MSE train: ', 0.0014538842316896268)\n",
      "('value compare', array([0.39925173, 0.77213351]), array([0.48558531, 0.78012084]), array([-0.36009029,  0.48012084]))\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "\n",
    "ANN = NeuralNetwork(network_structure=[input_size, 20,20, output_size],\n",
    "                               learning_rate=0.1,\n",
    "                               bias=True)\n",
    "    \n",
    "    \n",
    "ANN.train(train_X, train_Y, epochs=epochs)\n",
    "e, y_pred = ANN.evaluate(train_X, train_Y)\n",
    "print(\"value compare\", y_pred[46], train_Y[46],Y_train[46])\n",
    "#for i in range(0, y_pred.shape[0]):\n",
    "    #print(\"value compare\", y_pred[i], train_Y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  ANN.save_paras()\n",
    "import pickle\n",
    "\n",
    "with open('model.pkl', 'w') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump(model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.42052379]\n",
      " [ 0.47213351]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try_X=[[(120.0/320)*0.99+0.01-0.5, (119.0/240)*0.99+0.01-0.5]]\n",
    "try_x = np.asarray(try_X)\n",
    "#print try_x.shape\n",
    "pred = ANN.run(try_x)\n",
    "#pred = [0.39925173, 0.77213351]\n",
    "pred[0] = pred[0]*0.7-0.7\n",
    "pred[1] = pred[1]-0.3\n",
    "print pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('MSE: test', 0.0015450700496642607)\n",
      "('value compare', array([0.92728186, 0.31992131]), array([0.97361614, 0.30870003]))\n",
      "('value compare', array([0.80927598, 0.8071027 ]), array([0.83661896, 0.79963921]))\n",
      "('value compare', array([0.88064651, 0.88749494]), array([0.87152534, 0.88154571]))\n",
      "('value compare', array([0.83697029, 0.9034108 ]), array([0.76958824, 0.86492347]))\n",
      "('value compare', array([0.47459124, 0.46175739]), array([0.55046254, 0.47811772]))\n",
      "('value compare', array([0.9426821 , 0.44955952]), array([0.86853924, 0.38849011]))\n",
      "('value compare', array([0.1925835 , 0.30290111]), array([0.22924485, 0.3086998 ]))\n",
      "('value compare', array([0.71063154, 0.4078029 ]), array([0.71370061, 0.36552551]))\n",
      "('value compare', array([0.82261696, 0.36371303]), array([0.86554711, 0.32416997]))\n",
      "('value compare', array([0.78505123, 0.3870308 ]), array([0.78593166, 0.36441031]))\n",
      "('value compare', array([0.36221926, 0.88876067]), array([0.37005354, 0.88775759]))\n",
      "('value compare', array([0.12484125, 0.29829301]), array([0.11475011, 0.30870004]))\n",
      "('value compare', array([0.19502216, 0.34891076]), array([0.20793329, 0.34847329]))\n",
      "('value compare', array([0.9148065 , 0.77633837]), array([0.88327264, 0.76103331]))\n",
      "('value compare', array([0.93429362, 0.32170891]), array([0.98410492, 0.30870001]))\n",
      "('value compare', array([0.94182453, 0.76257674]), array([0.8978845 , 0.75354927]))\n",
      "('value compare', array([0.11133698, 0.30289105]), array([0.08456717, 0.30870001]))\n",
      "('value compare', array([0.80686665, 0.39230928]), array([0.82553008, 0.37821657]))\n",
      "('value compare', array([0.1343275 , 0.29905711]), array([0.10212943, 0.30869996]))\n",
      "('value compare', array([0.10446177, 0.29928789]), array([0.07890575, 0.32286834]))\n",
      "('value compare', array([0.68537532, 0.57988806]), array([0.6097064 , 0.52936493]))\n",
      "('value compare', array([0.656217  , 0.56756366]), array([0.7731768 , 0.63343387]))\n",
      "('value compare', array([0.91543644, 0.70529527]), array([0.87670613, 0.74711471]))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "e, y_pred = ANN.evaluate(test_X, test_Y)\n",
    "\n",
    "print(\"MSE: test\", e)\n",
    "for i in range(0, y_pred.shape[0]):\n",
    "    print(\"value compare\", y_pred[i], test_Y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
